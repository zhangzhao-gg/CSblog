1平台选AutoDL
2服务器配置选：vGPU-48GB
配置完实例后点击快捷工具里的JupterLab进去，进入终端。
先下载模型，模型记得放到数据盘里。
我们选择[这个Qwen3-30B的量化模型](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507-FP8)
下载指令为：
`pip install -U "huggingface[cli]`
`huggingface-cli download --resume-download Qwen/Qwen3-30B-A3B-Instruct-2507-FP8 --local-dir qwen3-30b-fp8`
`huggingface-cli download --resume-download QuantTrio/Qwen3-Coder-30B-A3B-Instruct-GPTQ-Int8 --local-dir qwen3-30b-int8`
`huggingface-cli download --resume-download Qwen/Qwen3-30B-A3B-GPTQ-Int4 --local-dir qwen3-30b-int4`
上面的下载指令改成你自己的模型和要下载到的文件夹下

下载可能中断，因为下载源在国外，我们进行换源
`export HF_ENDPOINT=https://hf-mirror.com`
换源之后应该可以正常下载。
下载完毕后
一行指令进行运行:我的模型是qwen3-30b-fp8，后面的最大上下文自己去对应界面看一下他支持的最大上下文，若太大会爆显存。
`vllm serve qwen3-30b-fp8 --max-model-len 32768 --port 6006` 
32768是32k,显存差不多是45G显存左右，刚好在48G以内。
运行起来后就在6006端口运行起来了。那么此时我们服务器里就有一个正在运行且部署到6006端口的服务了。
之后我们想要用其他电脑来调接口怎么办？
答案是我们用AutoDL的自定义服务，里面可以把端口映射到他们的子域名里。
以我为例：我的6006 端口映射到了
`https://xxxxxxxxxxxxxx.seetacloud.com:8443`

ok,下一步我们写客户端：
```
import os
import asyncio
import time
from openai import AsyncOpenAI
import httpx

# 本地 vLLM 服务地址
BASE_URL = "https://xxxxxxxxxxx.bjb1.seetacloud.com:8443/v1"
MODEL = "qwen3-30b-fp8/"

# 异步 HTTP client
http_client = httpx.AsyncClient(http2=True, timeout=180.0)
client = AsyncOpenAI(api_key="local_test_key", base_url=BASE_URL, http_client=http_client)

async def main():
    prompt = "请写一段关于秋天的短文。"

    # 记录发送时间
    send_time = time.time()

    # 异步流式调用
    stream = await client.chat.completions.create(
        model=MODEL,
        messages=[{"role": "user", "content": prompt}],
        stream=True,
        extra_body={
            "enable_thinking": False,
            "enable_search": False,
            "thinking": {"type": "disabled"},
        },
        stream_options={"include_usage": True},
    )

    first_token_time = None
    total_chars = 0
    start_output_time = None

    async for chunk in stream:
        if hasattr(chunk, "choices"):
            for choice in chunk.choices:
                if hasattr(choice.delta, "content"):
                    content = choice.delta.content
                    if content:
                        # 记录首包时间
                        if first_token_time is None:
                            first_token_time = time.time()
                            start_output_time = first_token_time
                            print(f"\n⚡ 首包延迟: {first_token_time - send_time:.2f} 秒\n")

                        # 输出内容
                        print(content, end="", flush=True)
                        total_chars += len(content)

    # 输出完毕后计算整体速度
    if start_output_time:
        end_time = time.time()
        total_time = end_time - start_output_time
        speed = total_chars / total_time if total_time > 0 else 0
        print(f"\n\n✅ 总生成字符数: {total_chars}")
        print(f"⏱️ 总用时: {total_time:.2f} 秒")
        print(f"⚙️ 平均生成速度: {speed:.2f} 字符/秒")

# 运行异步主函数
asyncio.run(main())
```
用 python快速搭建一个客户端来调用服务